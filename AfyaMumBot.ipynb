{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Title: AfyaMum Bot: AI-powered pregnancy support chatbot\n",
    "\n",
    "## Introduction\n",
    "This notebook utilizes the power of Llama 2, an open-source large language model, in combination with a comprehensive document on Maternal Reproductive Health to offer expectant mothers tailored information and support. Maternal health during pregnancy is of utmost importance, and this notebook aims to empower expectant mothers with personalized insights, risk assessment, prevention methods, and guidance for a healthier pregnancy journey.\n",
    "\n",
    "## How it works?\n",
    "1. **Data Input**: Expectant mothers provide information on how they feel through Whatsapp\n",
    "\n",
    "2. **Personalization**: Llama 2 will process the input data to generate personalized recommendations and assessments.\n",
    "\n",
    "The Personalized Maternal Reproductive Health Guide, powered by Llama-2, aims to provide expectant mothers with the knowledge, tools, and support needed for a healthy and informed pregnancy journey.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install python-dotenv\n",
    "#!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install chromadb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using OpenAI's GPT-3.5 to do prompt engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.0: Loading the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-3.5-turbo\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "current_date = datetime.datetime.now().date()\n",
    "if current_date < datetime.date(2023, 9, 2):\n",
    "    llm_name = \"gpt-3.5-turbo-0301\"\n",
    "else:\n",
    "    llm_name = \"gpt-3.5-turbo\"\n",
    "print(llm_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "langchain_tracing_v2 = os.environ['LANGCHAIN_TRACING_V2']\n",
    "langchain_endpoint = os.environ['LANGCHAIN_ENDPOINT']\n",
    "langchain_api_key = os.environ['LANGCHAIN_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "persist_directory = 'docs/chroma/'\n",
    "embedding = OpenAIEmbeddings()\n",
    "vectordb = Chroma(persist_directory=persist_directory, embedding_function=embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! How can I assist you today?'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "llm = ChatOpenAI(model_name=llm_name, temperature=0)\n",
    "llm.predict(\"Hello!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The major causes of pregnancy complications can include pre-existing health conditions, such as diabetes or high blood pressure, infections during pregnancy, and problems with the placenta or umbilical cord. Thanks for asking and I'm optimistic everything will be well!\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build prompt\n",
    "from langchain.prompts import PromptTemplate\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Use three sentences maximum. Keep the answer as concise as possible. Always say \"Thanks for asking and I'm optimistic everything will be well!\" at the end of the answer. \n",
    "{context}\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "QA_CHAIN_PROMPT = PromptTemplate(input_variables=[\"context\", \"question\"],template=template,)\n",
    "\n",
    "# Run chain\n",
    "from langchain.chains import RetrievalQA\n",
    "question = \"What are the major causes of pregnancy complications?\"\n",
    "qa_chain = RetrievalQA.from_chain_type(llm,\n",
    "                                       retriever=vectordb.as_retriever(),\n",
    "                                       return_source_documents=True,\n",
    "                                       chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT})\n",
    "\n",
    "\n",
    "result = qa_chain({\"query\": question})\n",
    "result[\"result\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1: Storing chat history in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2: Implementing ConversationalRetrievalChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "retriever=vectordb.as_retriever()\n",
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "    llm,\n",
    "    retriever=retriever,\n",
    "    memory=memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Is infections during a pregnancy a major cause of pregnancy complications and what number of expectant women encounter these complications?\"\n",
    "result = qa({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Infections during pregnancy can indeed be a major cause of pregnancy complications. Certain infections, such as urinary tract infections, bacterial vaginosis, and sexually transmitted infections, can increase the risk of preterm labor, premature rupture of membranes, and low birth weight. Other infections, like rubella, cytomegalovirus, and Zika virus, can lead to birth defects in the baby.\\n\\nAs for the number of expectant women who encounter these complications, it can vary depending on various factors such as geographic location, socioeconomic status, access to healthcare, and individual health conditions. It is challenging to provide an exact number without specific data or statistics. It is always recommended for pregnant women to seek regular prenatal care and follow the guidance of healthcare professionals to minimize the risk of infections and associated complications.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Which infections among the following: urinary tract infections, bacterial vaginosis, and sexually transmitted infections is the major cause of complications in pregnancy?\"\n",
    "result = qa({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Among the infections listed, sexually transmitted infections (STIs) are generally considered to be the major cause of complications in pregnancy. STIs such as chlamydia, gonorrhea, syphilis, and HIV can pose significant risks to both the mother and the developing fetus if left untreated. It is important for pregnant individuals to receive regular prenatal care and be tested for STIs to prevent complications.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['answer']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Llama 2 Model to develop AfyaMum Bot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Develop AfyaMum Bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import DocArrayInMemorySearch\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.chains import RetrievalQA,  ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain import PromptTemplate\n",
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install ctransformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import CTransformers\n",
    "# Loading the model\n",
    "def load_llm():\n",
    "    # Load the locally downloaded model here\n",
    "    llm = CTransformers(\n",
    "        model = \"llama-2-7b-chat.ggmlv3.q8_0.bin\",\n",
    "        model_type=\"llama\",\n",
    "        max_new_tokens = 512,\n",
    "        temperature = 0.5\n",
    "    )\n",
    "    return llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_prompt_template = \"\"\"Use the following pieces of information to answer the user's question in a simple manner which will enable him or her to easily understand.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\n",
    "Only return the helpful answer, risk level e.g high, medium or low risk, and possible remedies if low risk involved\n",
    "below and nothing else.\n",
    "Helpful answer for expectant mothers:\n",
    "\n",
    "Symptom Assessment: Provide an assessment of the symptoms described in the query.\n",
    "\n",
    "Risk Evaluation: Evaluate the potential risks and complications expectant mothers may be predisposed to based on the conditions presented in the query.\n",
    "\n",
    "Personalized Guidance: Offer recommended actions, preventive measures, and self-care tips tailored to the user's specific situation.\n",
    "\n",
    "Specialist Referral: Indicate whether specialized medical attention may be needed and, if so, provide guidance on seeking such care.\n",
    "\n",
    "Risk Level: Assess the risk level associated with the symptoms or conditions mentioned (e.g., high, medium, or low risk).\n",
    "\n",
    "Possible Remedies (for Low Risk): If the risk level is low, suggest appropriate remedies or self-care steps.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_custom_prompt():\n",
    "    \"\"\"\n",
    "    Prompt template for ConversationalRetrievalQAChain\n",
    "    \"\"\"\n",
    "    prompt = PromptTemplate(template=custom_prompt_template,\n",
    "                            input_variables=['context', 'question'])\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_FAISS_PATH = 'vectorstore/db_faiss'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install faiss_cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vector_db(file):\n",
    "    # load documents\n",
    "    loader = PyPDFLoader(file)\n",
    "    documents = loader.load()\n",
    "    # split documents\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "    docs = text_splitter.split_documents(documents)\n",
    "    # define embedding\n",
    "    embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2', model_kwargs={'device': 'cpu'})\n",
    "    # create vector database from data\n",
    "    # db = DocArrayInMemorySearch.from_documents(docs, embeddings)\n",
    "    db = FAISS.from_documents(docs, embeddings)\n",
    "    db.save_local(DB_FAISS_PATH)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/AfyaMum-bot/.conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "create_vector_db(\"./Data/Oxford_Gynecology.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ConversationalRetrievalChain\n",
    "def conversational_retrieval_chain(llm, prompt, db, k, chain_type):\n",
    "    # create a chatbot chain. Memory is managed externally.\n",
    "    # define retriever\n",
    "    retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": k})\n",
    "    qa = ConversationalRetrievalChain.from_llm(\n",
    "        llm=llm, \n",
    "        chain_type=chain_type, \n",
    "        retriever=retriever, \n",
    "        return_source_documents=True,\n",
    "        return_generated_question=True,\n",
    "        combine_docs_chain_kwargs={'prompt': prompt}\n",
    "    )\n",
    "    return qa "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import param\n",
    "\n",
    "class cbfs(param.Parameterized):\n",
    "    chat_history = param.List([])\n",
    "    answer = param.String(\"\")\n",
    "    db_query = param.String(\"\")\n",
    "    db_response = param.List([])\n",
    "\n",
    "    def __init__(self, **params):\n",
    "        super(cbfs, self).__init__(**params)\n",
    "        # self.loaded_file = \"./Data/Gynecology.pdf\"\n",
    "        # self.qa = load_db(self.loaded_file, \"stuff\", 4)\n",
    "        llm = load_llm()\n",
    "        prompt = set_custom_prompt()\n",
    "        embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2', model_kwargs={'device': 'cpu'})\n",
    "        db = FAISS.load_local(DB_FAISS_PATH, embeddings)\n",
    "        self.qa = conversational_retrieval_chain(llm, prompt, db, 4, \"stuff\")\n",
    "\n",
    "    def convchain(self, query):\n",
    "        result = self.qa({\"question\": query, \"chat_history\": self.chat_history})\n",
    "        self.chat_history.extend([(query, result[\"answer\"])])\n",
    "        self.db_query = result[\"generated_question\"]\n",
    "        self.db_response = result[\"source_documents\"]\n",
    "        self.answer = result['answer']\n",
    "        return self.answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  AfyaMum Bot Instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install \"langchain[docarray]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'During preterminal cord is a velamented in the placentralges well. Fetal cord extends into the vasa preterminal cord passes over the umbilaterally displacerv\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nBleakin place the fetal vessels traverse the fetal vessels, but does not present between  The following are essential for the umbilamin. or  What can help@ If the cervagain, or velamentary exposed through a velamented. A few mmembarranged in the placentrstretcheducation of vasa preterminal cord is not present on the membraneously found anteriorly soft (Mosted.\\n\\n\\n\\n\\n\\n\\n\\nIf you should consultation to ensure that is present over the umbilaterally absent or when placent of blood from or percepticants, or other types of the presence. It may cause\\nIt has an access.\\n\\n\\n\\n\\n\\nDuring preterm birth defective, or without the fetal cord is a) The placentrstretched, if you can occur in contact your healthcarefulc. In addition'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cb = cbfs()\n",
    "cb.convchain(\"What is preclampsia?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
